{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s916fmRwjUBP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Layer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Rescaling , GlobalAveragePooling2D\n",
        "from tensorflow.keras import layers, optimizers, callbacks\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.applications import EfficientNetV2B2\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fez7xUJnv-2"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBlB_qM6n3uO"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Replace filename if it's different\n",
        "zip_filename = \"TrashType_Image_Dataset.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"TrashData\")\n",
        "\n",
        "# Check folder structure\n",
        "print(os.listdir(\"TrashData\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuxSz8X-kc8J"
      },
      "outputs": [],
      "source": [
        "dataset_dir= r\"TrashData/TrashType_Image_Dataset\"\n",
        "image_size = (124, 124)\n",
        "batch_size = 32\n",
        "seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cashMGxOkjVv"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=seed,\n",
        "    shuffle = True,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTc8B6IJklIi"
      },
      "outputs": [],
      "source": [
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=seed,\n",
        "    shuffle = True,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "val_class= val_ds.class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A-GkiAioBMK"
      },
      "outputs": [],
      "source": [
        "# Get the total number of batches in the validation dataset\n",
        "val_batches = tf.data.experimental.cardinality(val_ds)\n",
        "\n",
        "# Split the validation dataset into two equal parts:\n",
        "# First half becomes the test dataset\n",
        "test_ds = val_ds.take(val_batches // 2)\n",
        "\n",
        "# Second half remains as the validation dataset\n",
        "val_dat = val_ds.skip(val_batches // 2)\n",
        "\n",
        "# Optimize test dataset by caching and prefetching to improve performance\n",
        "test_ds_eval = test_ds.cache().prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ncN4YixoGkl"
      },
      "outputs": [],
      "source": [
        "print(train_ds.class_names)\n",
        "print(val_class)\n",
        "print(len(train_ds.class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO80NygvoJCw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(12):\n",
        "    ax = plt.subplot(4, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(train_ds.class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybKPYCLfoK3q"
      },
      "outputs": [],
      "source": [
        "def count_distribution(dataset, class_names):\n",
        "    counts = dict.fromkeys(class_names, 0)\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in dataset:\n",
        "        for label in labels.numpy():\n",
        "            counts[class_names[label]] += 1\n",
        "            total += 1\n",
        "\n",
        "    if total == 0:\n",
        "        return {k: 0 for k in class_names}  # Avoid division by zero\n",
        "\n",
        "    for k in counts:\n",
        "        counts[k] = round((counts[k] / total) * 100, 2)  # Convert to percentage\n",
        "    return counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OZb4uV7oNV6"
      },
      "outputs": [],
      "source": [
        "# Function to plot class distribution\n",
        "def simple_bar_plot(dist, title):\n",
        "    plt.bar(dist.keys(), dist.values(), color='cornflowerblue')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Percentage (%)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylim(0, 100)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MomdFdj5oPMl"
      },
      "outputs": [],
      "source": [
        "class_names = train_ds.class_names\n",
        "\n",
        "# Get class distributions\n",
        "train_dist = count_distribution(train_ds, class_names)\n",
        "val_dist = count_distribution(val_ds, class_names)\n",
        "test_dist = count_distribution(test_ds, class_names)\n",
        "overall_dist = {}\n",
        "for k in class_names:\n",
        "    overall_dist[k] = round((train_dist[k] + val_dist[k]) / 2, 2)\n",
        "\n",
        "print(train_dist)\n",
        "print(val_dist)\n",
        "print(test_dist)\n",
        "print(overall_dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jeFr-e6oQ7Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Show visualizations\n",
        "simple_bar_plot(train_dist, \"Training Set Class Distribution (%)\")\n",
        "simple_bar_plot(val_dist, \"Validation Set Class Distribution (%)\")\n",
        "simple_bar_plot(test_dist, \"Test Set Class Distribution (%)\")\n",
        "simple_bar_plot(overall_dist, \"Overall Class Distribution (%)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Count class occurrences and prepare label list\n",
        "class_counts = {i: 0 for i in range(len(class_names))}\n",
        "all_labels = []\n",
        "\n",
        "for images, labels in train_ds:\n",
        "    for label in labels.numpy():\n",
        "        class_counts[label] += 1\n",
        "        all_labels.append(label)\n",
        "\n",
        "# Compute class weights (index aligned)\n",
        "class_weights_array = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.arange(len(class_names)),\n",
        "    y=all_labels\n",
        ")\n",
        "\n",
        "# Create dictionary mapping class index to weight\n",
        "class_weights = {i: w for i, w in enumerate(class_weights_array)}\n"
      ],
      "metadata": {
        "id": "UC1LWs6VzXqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ✅ Optional: print results\n",
        "print(\"Class Counts:\", class_counts)\n",
        "print(\"Class Weights:\", class_weights)\n"
      ],
      "metadata": {
        "id": "BajF6iU7zZz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Define data augmentation pipeline\n",
        "data_augmentation = Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomContrast(0.1),\n",
        "])"
      ],
      "metadata": {
        "id": "wjDXLAIAzbsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Load the pretrained MobileNetV3Small model (without the top classification layer)\n",
        "base_model = EfficientNetV2B2(include_top=False, input_shape=(124, 124, 3),include_preprocessing=True, weights='imagenet')\n",
        "\n",
        "\n",
        "#  Freeze early layers (to retain general pretrained features)\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:100]:  # You can adjust this number\n",
        "    layer.trainable = False\n",
        "\n"
      ],
      "metadata": {
        "id": "X0oDADxFzdON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Build the final model\n",
        "model = Sequential([\n",
        "    layers.Input(shape=(124, 124, 3)),\n",
        "    data_augmentation,\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(6, activation='softmax')  # Change to your number of classes\n",
        "])\n"
      ],
      "metadata": {
        "id": "ERaHKrPsze0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⚙️ Compile the model\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ],
      "metadata": {
        "id": "HvRbT3FBzgr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an EarlyStopping callback to stop training when validation loss stops improving\n",
        "early = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',            # Metric to monitor (validation loss here)\n",
        "    patience=3,                   # Number of epochs to wait after last improvement before stopping\n",
        "    restore_best_weights=True     # After stopping, restore the model weights from the epoch with the best val_loss\n",
        ")"
      ],
      "metadata": {
        "id": "UTWcxjGZzimu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of epochs to train the model\n",
        "epochs = 15  # Number of times the model will go through the entire dataset\n",
        "\n",
        "# Train the model using the fit function\n",
        "history = model.fit(\n",
        "    train_ds,                # Training dataset used to adjust model weights\n",
        "    validation_data=val_ds,   # Validation dataset to monitor performance on unseen data\n",
        "    epochs=epochs,           # Number of training cycles, referencing the variable set earlier\n",
        "    class_weight=class_weights,  # Handles class imbalances by assigning appropriate weights\n",
        "    batch_size=32,           # Number of samples processed in each training step\n",
        "    callbacks=[early]        # Implements early stopping to prevent unnecessary training\n",
        ")"
      ],
      "metadata": {
        "id": "SI3UnNnbzkqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📝 Summary (optional but useful)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "C20b1XM4zmZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary() # Print the architecture summary of the base model"
      ],
      "metadata": {
        "id": "0RquzHXgzqOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']          # Extract training accuracy from history\n",
        "val_acc = history.history['val_accuracy']  # Extract validation accuracy from history\n",
        "loss = history.history['loss']             # Extract training loss from history\n",
        "val_loss = history.history['val_loss']     # Extract validation loss from history\n",
        "\n",
        "epochs_range = range(len(acc))             # Define range for epochs based on accuracy length\n",
        "\n",
        "plt.figure(figsize=(10,8))                 # Set overall figure size for visualization\n",
        "\n",
        "plt.subplot(1,2,1)                         # Create first subplot (1 row, 2 columns, position 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')       # Plot training accuracy\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy') # Plot validation accuracy\n",
        "plt.legend(loc='lower right')              # Place legend in lower-right corner\n",
        "plt.title('Training vs Validation Accuracy') # Add title for accuracy plot\n",
        "\n",
        "plt.subplot(1,2,2)                         # Create second subplot (1 row, 2 columns, position 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')         # Plot training loss\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')   # Plot validation loss\n",
        "plt.legend(loc='upper right')              # Place legend in upper-right corner\n",
        "plt.title('Training vs Validation Loss')   # Add title for loss plot\n",
        "\n",
        "plt.show()                                 # Display the plots"
      ],
      "metadata": {
        "id": "b_qTukvPzrpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_ds_eval)\n",
        "print(f'Test accuracy is{accuracy:.4f}, Test loss is {loss:.4f}')"
      ],
      "metadata": {
        "id": "BjjKaz9aztfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract true labels from all batches in the test dataset\n",
        "y_true = np.concatenate([y.numpy() for x, y in test_ds_eval], axis=0)  # Convert Tensor labels to NumPy array and concatenate them\n",
        "\n",
        "# Get predictions as probabilities from the model\n",
        "y_pred_probs = model.predict(test_ds_eval)  # Predict class probabilities for each sample in the test dataset\n",
        "\n",
        "# Convert probabilities to predicted class indices\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)  # Select the class with the highest probability for each sample\n",
        "\n",
        "# Compute the confusion matrix to evaluate classification performance\n",
        "cm = confusion_matrix(y_true, y_pred)  # Generate confusion matrix comparing true labels to predicted labels\n",
        "\n",
        "# Print metrics to assess model performance\n",
        "print(cm)  # Display confusion matrix\n",
        "print(classification_report(y_true, y_pred))  # Print precision, recall, and F1-score for each class\n"
      ],
      "metadata": {
        "id": "XIGDU7z_zvIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))  # Set figure size for better visualization\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d',  # Create heatmap using confusion matrix\n",
        "            xticklabels=class_names,  # Set class names for x-axis (predicted labels)\n",
        "            yticklabels=class_names,  # Set class names for y-axis (true labels)\n",
        "            cmap='Blues')  # Use a blue colormap for better contrast\n",
        "\n",
        "plt.xlabel('Predicted')  # Label x-axis as Predicted classes\n",
        "plt.ylabel('True')  # Label y-axis as True classes\n",
        "plt.title('Confusion Matrix')  # Add title to the heatmap\n",
        "plt.show()  # Display the plot"
      ],
      "metadata": {
        "id": "2iwdWgObzwl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract class names from the training dataset\n",
        "class_names = train_ds.class_names\n",
        "\n",
        "# Take one batch of images and labels from the test dataset for evaluation\n",
        "for images, labels in test_ds_eval.take(1):\n",
        "\n",
        "    # Generate predictions for the batch of images\n",
        "    predictions = model.predict(images)\n",
        "\n",
        "    # Get the predicted class index for each image\n",
        "    pred_labels = tf.argmax(predictions, axis=1)\n",
        "\n",
        "    # Loop through the first 8 images in the batch\n",
        "    for i in range(8):\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))  # Convert and display image\n",
        "        plt.title(f\"True: {class_names[labels[i]]}, Pred: {class_names[pred_labels[i]]}\")  # Show actual and predicted class\n",
        "        plt.axis(\"off\")  # Hide axes for better visualization\n",
        "        plt.show()  # Display the image with title"
      ],
      "metadata": {
        "id": "uhSh5HfQzych"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model in Keras format with architecture, weights, and training configuration\n",
        "model.save('Effiicientnetv2b2.keras')\n",
        "\n",
        "# Load your Keras model\n",
        "model = tf.keras.models.load_model('Effiicientnetv2b2.keras')"
      ],
      "metadata": {
        "id": "QYIAqTiPz02W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('Effiicientnetv2b2.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nAbapQ_7ZpiJ",
        "outputId": "0e79ed11-d4aa-4cd8-dd95-17c4227fa740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6cb810c0-692f-4311-a35a-4c3d4b5a8b04\", \"Effiicientnetv2b2.keras\", 100313321)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "BNRPRqSm0DuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input"
      ],
      "metadata": {
        "id": "W7DRenDd1ohI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_image(img):\n",
        "    # Resize image to 124x124 pixels (Note: Comment says 128x128, but code resizes to 124x124)\n",
        "    img = img.resize((124, 124))\n",
        "\n",
        "    # Convert image to a NumPy array with float32 dtype\n",
        "    img_array = np.array(img, dtype=np.float32)\n",
        "    img_array = preprocess_input(img_array)\n",
        "\n",
        "    # Expand dimensions to match model input shape (adds a batch dimension)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Make a prediction using the trained model\n",
        "    prediction = model.predict(img_array)\n",
        "\n",
        "    # Get the index of the highest predicted probability\n",
        "    predicted_class_index = np.argmax(prediction)\n",
        "\n",
        "    # Map the predicted index to its corresponding class name\n",
        "    predicted_class_name = class_names[predicted_class_index]\n",
        "\n",
        "    # Extract confidence score (probability of the predicted class)\n",
        "    confidence = prediction[0][predicted_class_index]\n",
        "\n",
        "    # Return formatted prediction result with confidence score\n",
        "    return f\"Predicted: {predicted_class_name} (Confidence: {confidence:.2f})\""
      ],
      "metadata": {
        "id": "ZX9ycXzK1ray"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=classify_image,  # Function to classify image using the trained model\n",
        "    inputs=gr.Image(type=\"pil\"),  # Accepts input as a PIL image\n",
        "    outputs=\"text\"  # Outputs prediction as text\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()  # Start the Gradio interface for user interaction"
      ],
      "metadata": {
        "id": "HNWRfusS1tQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Upload image\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Class labels\n",
        "class_names = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
        "\n",
        "for file_name in uploaded.keys():\n",
        "    img = Image.open(file_name).convert(\"RGB\")\n",
        "\n",
        "    # Resize to match your model’s expected input shape\n",
        "    img_resized = img.resize((124, 124))  # Change to 224 if your model used that\n",
        "    img_array = np.array(img_resized) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(img_array)[0]\n",
        "\n",
        "    # Display image\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Uploaded Image\")\n",
        "    plt.show()\n",
        "\n",
        "    # Display all class probabilities\n",
        "    print(\"\\n🔍 Prediction Breakdown:\")\n",
        "    for i, prob in enumerate(prediction):\n",
        "        print(f\"{class_names[i]:<10}: {prob * 100:.2f}%\")\n",
        "\n",
        "    # Display top prediction\n",
        "    top_class = class_names[np.argmax(prediction)]\n",
        "    top_confidence = np.max(prediction) * 100\n",
        "    print(f\"\\n✅ Predicted: **{top_class}** with {top_confidence:.2f}% confidence\")\n"
      ],
      "metadata": {
        "id": "kkrWs5rw1vsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YxDTqSV4240b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}